{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa7671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097f2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = dict(n_jobs=-1, n_estimators=256, min_samples_leaf=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714bc33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032432</td>\n",
       "      <td>-0.013066</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>0.035532</td>\n",
       "      <td>-0.014728</td>\n",
       "      <td>-0.014885</td>\n",
       "      <td>-0.005247</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007763</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>-0.017136</td>\n",
       "      <td>-0.025479</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.032405</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>-0.005976</td>\n",
       "      <td>-0.016520</td>\n",
       "      <td>-1.007347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008624</td>\n",
       "      <td>-0.032773</td>\n",
       "      <td>-0.020031</td>\n",
       "      <td>0.020145</td>\n",
       "      <td>-0.006798</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>-0.040737</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031387</td>\n",
       "      <td>-0.014356</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.038014</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>-0.004310</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.016031</td>\n",
       "      <td>16.710669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015039</td>\n",
       "      <td>0.034703</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.015583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001197</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>-0.032698</td>\n",
       "      <td>-0.021573</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.021787</td>\n",
       "      <td>-4.883995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009395</td>\n",
       "      <td>0.024268</td>\n",
       "      <td>-0.031028</td>\n",
       "      <td>-0.029758</td>\n",
       "      <td>-0.040502</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.017631</td>\n",
       "      <td>-0.034498</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005521</td>\n",
       "      <td>-0.001306</td>\n",
       "      <td>-0.009699</td>\n",
       "      <td>-0.007910</td>\n",
       "      <td>0.025553</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>-0.008468</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-3.864474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>-0.012750</td>\n",
       "      <td>-0.013199</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.017628</td>\n",
       "      <td>-0.001793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>-0.037190</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>0.005426</td>\n",
       "      <td>-0.007129</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.004865</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>6.945772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.032432 -0.013066  0.004673 -0.017611  0.035532 -0.014728 -0.014885   \n",
       "1  0.008624 -0.032773 -0.020031  0.020145 -0.006798  0.003736  0.023822   \n",
       "2 -0.015039  0.034703  0.031771  0.001136 -0.000144  0.041739 -0.010032   \n",
       "3  0.009395  0.024268 -0.031028 -0.029758 -0.040502  0.029972  0.011669   \n",
       "4  0.003794  0.009415  0.006692 -0.012750 -0.013199 -0.006064  0.015940   \n",
       "\n",
       "          7         8         9  ...        21        22        23        24  \\\n",
       "0 -0.005247  0.003580 -0.008661  ... -0.007763  0.016018 -0.017136 -0.025479   \n",
       "1 -0.040737  0.025638 -0.004040  ... -0.031387 -0.014356  0.001716  0.038014   \n",
       "2  0.005467  0.003280  0.015583  ... -0.001197  0.028461 -0.032698 -0.021573   \n",
       "3  0.017631 -0.034498  0.003571  ...  0.005521 -0.001306 -0.009699 -0.007910   \n",
       "4  0.023851  0.017628 -0.001793  ...  0.010429 -0.000769 -0.037190  0.039113   \n",
       "\n",
       "         25        26        27        28        29     target  \n",
       "0 -0.038388 -0.032405 -0.004957 -0.005976 -0.016520  -1.007347  \n",
       "1  0.016288  0.008245 -0.004310  0.001984  0.016031  16.710669  \n",
       "2  0.013873  0.024355 -0.015747  0.005714  0.021787  -4.883995  \n",
       "3  0.025553 -0.048658 -0.008468  0.045984  0.004110  -3.864474  \n",
       "4  0.005426 -0.007129  0.001969  0.004865 -0.020894   6.945772  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toy_dataset(n_train):\n",
    "    # Generating an unbalanced dataset\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_train,\n",
    "        n_features=30,\n",
    "        n_informative=5,\n",
    "        effective_rank=12,\n",
    "        tail_strength=0.5,\n",
    "        noise=10.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    df = pd.DataFrame(X)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    _X = df.columns\n",
    "    _y = 'target'\n",
    "    df[_y] = y\n",
    "    return df, _X, _y\n",
    "\n",
    "df, _X, _y = generate_toy_dataset(n_train=1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd2fa0",
   "metadata": {},
   "source": [
    "# Bootstrapping and the Out of Bag Sample\n",
    "\n",
    "By default in scikit-learn, the Random Forest Regressor uses boostrap sampling.  That is, if you feed in \"n\" samples, every decision tree in the forest is trained on \"n\" samples chosen with replacement from the original \"n\".  \n",
    "\n",
    "Since the sampling happens with replacement, there will almost always be some samples left out of training on each tree.  Those rows are described as being \"out of bag\".\n",
    "\n",
    "The \"out of bag\" sample is interesting because it has the properties of a randomly selected validation set.  For each estimator added to the forest we have a \"free\" validation set sitting there which can be used in a manner similar to other forms of validation like \"leave one out\" or \"k-fold cross validation\".\n",
    "\n",
    "# oob_prediction\n",
    "\n",
    "We can form a prediction for the out of bag samples simply by feeding them into the decision tree that was trained during the round in which they were left out of training.\n",
    "\n",
    "These predictions are generated for each estimator, so in general each sample will have many out-of-bag predictions.\n",
    "\n",
    "The standard way of producing a prediction from a random forest is to feed a sample into each decision tree, get a prediction from each, and then average all of the predictions together.In a similar way, we can form an \"out-of-bag prediction\" for each sample in the training set by averaging together all of the individual out-of-bag predictions for that sample.  \n",
    "\n",
    "Since a given sample was never involved in the training of the tree that produces its out of bag prediction, the out-of-bag prediction can provide an estimate of generalization to unseen samples. In fact, it can be proven that the out-of-bag prediction is in general a *slightly pessimistic* evaluation of generalization performance.\n",
    "\n",
    "# The oob_prediction is like having a \"free\" set of cross-validation predictions\n",
    "\n",
    "The best thing about the oob_prediction is that it comes \"for free\" (small computational cost) as long as you ask for it, simply by specifying \"oob_score=True\". \n",
    "\n",
    "Do that and train the model and you'll be given access to \"oob_prediction\" which you can use to compute out-of-bag statistics which are very similar to cross validation statistics.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4be54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=20, n_estimators=256, n_jobs=-1,\n",
       "                      oob_score=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor(oob_score=True, **rf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd2b55",
   "metadata": {},
   "source": [
    "# Compare oob to xval\n",
    "\n",
    "I'd like to argue that OOB Validation is simply generally better than KFold Cross Validation\n",
    "\n",
    "Why it's better:\n",
    "- it's easier and simpler to code up. \n",
    "- it's more precise (performance metric estimates have less variation)\n",
    "- train only 1 model instead of k models.  It's k times faster!  really it's k+1 times faster!\n",
    "\n",
    "In addition, there are downstream tasks like stacking, calibration, and potentially more - which are easier to do with OOB predictions than with a kfold-based approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f3f3c",
   "metadata": {},
   "source": [
    "# The code is simpler than kfold\n",
    "\n",
    "The code is just easier to write.  I don't know about you - but for me there's always a little bit of mental overhead or boilerplate in setting up a kfold loop or using a cross-val convenience wrapper.  OOB just totally eliminates that.\n",
    "\n",
    "No loops, no cross-val wrappers - just a single extra param and you've got your OOB predictions ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f6e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_mse(additional_rf_params={}):\n",
    "    model = RandomForestRegressor(oob_score=True, **{**rf_params, **additional_rf_params})\n",
    "    P = model.fit(df[_X], df[_y]).oob_prediction_ #boom - one and done\n",
    "    return mean_squared_error(P, df[_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8accb",
   "metadata": {},
   "source": [
    "### Compare to Kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3c4db",
   "metadata": {},
   "source": [
    "Compare to using \"KFold\" + manually indexing folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6352e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold_mse(n_folds):\n",
    "    mses = []\n",
    "    for train_index, val_index in KFold(n_splits=n_folds, random_state=None, shuffle=True).split(df):\n",
    "        X_train = df.loc[train_index, _X]\n",
    "        y_train = df.loc[train_index, _y]\n",
    "        X_val   = df.loc[val_index, _X]\n",
    "        y_val   = df.loc[val_index, _y]\n",
    "        model = RandomForestRegressor(oob_score=False, **rf_params)\n",
    "        P_val = model.fit(X_train, y_train).predict(X_val)\n",
    "        mses.append(mean_squared_error(P_val, y_val))\n",
    "    return np.mean(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac145567",
   "metadata": {},
   "source": [
    "There's a lot to track here in terms of indexing etc. \n",
    "\n",
    "To be fair, there are things you can do with this approach that you can't do with the OOB technique.  For example: stratified sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1e1d2",
   "metadata": {},
   "source": [
    "### Compare to Xval Convenience Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6e47a",
   "metadata": {},
   "source": [
    "Or compare to scikit-learn's cross validation convenience functions which can be less elegant and less flexible than the OOB approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70973640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "def sklearn_cv_mse(n_folds):\n",
    "    mse = make_scorer(mean_squared_error)\n",
    "    model = RandomForestRegressor(oob_score=False, **rf_params)\n",
    "    cv_results = cross_validate(model, df[_X], df[_y], cv=n_folds, scoring=mse) \n",
    "    return np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add623bb",
   "metadata": {},
   "source": [
    "In this example you need to remember to make your own \"mean_squared_error\" scorer using \"make_scorer\" or you have to feed in \"neg_mean_squared_error\" which results in the output score being negative.\n",
    "\n",
    "It's a bit awkward!  \n",
    "\n",
    "I went with making my own scorer to avoid any downstream effects from potentially forgetting to negate the output of cv_results['test_score'] (for example - accidentally selecting the worst model instead of the best model from a hyperparam search).\n",
    "\n",
    "Even the somewhat streamlined cross_validate convenience function isn't as clean, simple, and terse as the OOB validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c79fbe",
   "metadata": {},
   "source": [
    "# OOB Validation is more precise than kfold cross-validation\n",
    "\n",
    "Let's measure the precision of out of bag validation compared to kfold cross validation .\n",
    "\n",
    "We'll do this by repeatedly running en experiment in which we compute a validation statistic (mean squared error) and then analyze the variation of that statistic across repeated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f4fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c5cee3d16343dc9756fb65f979b974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def oob_vs_kfold():\n",
    "    n_experiment_rounds = 50\n",
    "    \n",
    "    oob_mses      = [oob_mse() for _ in tqdm(range(n_experiment_rounds))]\n",
    "    kfold_mses_3  = [kfold_mse(n_folds=3)  for _ in tqdm(range(n_experiment_rounds))]\n",
    "    kfold_mses_5  = [kfold_mse(n_folds=5)  for _ in tqdm(range(n_experiment_rounds))]\n",
    "    kfold_mses_10 = [kfold_mse(n_folds=10) for _ in tqdm(range(n_experiment_rounds))]\n",
    "    kfold_mses_20 = [kfold_mse(n_folds=20) for _ in tqdm(range(n_experiment_rounds))]\n",
    "    \n",
    "    \n",
    "    mses = pd.DataFrame(dict(oob=oob_mses, \n",
    "                             kfold_3=kfold_mses_3,\n",
    "                             kfold_5=kfold_mses_5,\n",
    "                             kfold_10=kfold_mses_10,\n",
    "                             kfold_20=kfold_mses_20\n",
    "                            ))\n",
    "    \n",
    "    return mses\n",
    "    \n",
    "mses = oob_vs_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e424af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e93621",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses.agg(['mean', 'std', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b229841",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses.plot(kind='hist', bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a800b",
   "metadata": {},
   "source": [
    "You can see from these stats as well as the histogram that each of these methods converge on a very similar estimate of the mean squared error.\n",
    "\n",
    "However, the out of bag estimate exhibits lower variance than each of the kfold methods.\n",
    "\n",
    "While tuning a model - those kinds of swings in the estimation of the model performance can have an influence on hyperparameter decisions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1246b23",
   "metadata": {},
   "source": [
    "# OOB Validation is WAY Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abfe22",
   "metadata": {},
   "source": [
    "Even at 20 folds, our kfold estimate of the MSE has more variation than the OOB estimate - and it takes 20 times as long to compute!\n",
    "\n",
    "In general, you can think of OOB validation as only requiring \"1x\" computation while kfold cross validation requires \"kx\"- that is, it takes k times as long or k times as much compute.\n",
    "\n",
    "That can be a HUGE difference in compute time.\n",
    "\n",
    "If you want to squeeze all you can out of your model while tuning, out of bag is often a superior option - being either much more stable than comparably fast kfold estimates (though it's still more than twice as fast as 3-fold x-val), or an order of magnitude faster than comparably stable kfold estimates.\n",
    "\n",
    "The impact of this can't be overestimated - having your experiments run 2-20x faster can be transformative in terms of your ability to iterate on a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d512c",
   "metadata": {},
   "source": [
    "# Bonus 1: One more time!\n",
    "\n",
    "In addition, you'll probably want to retrain the model one more time after performing kfold x-val.  This is because your kfold models will each have only been trained on a subset of the data.  \n",
    "\n",
    "You could just use one of the models from one of the folds, but depending on how many folds you've chosen you might be missing out on e.g. 33%, 25%, 20% etc of the dataset. That's usually enough to make a difference.\n",
    "\n",
    "This isn't necessarily the case with OOB.  If you know what hyperparams you're using (or you have the model sitting around after training), it's already trained after having performed OOB validation - there's no need to train it again.  It's already been trained on all of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234d774",
   "metadata": {},
   "source": [
    "# Bonus 2: Its even more precise as you crank up the number of estimators\n",
    "\n",
    "As you add estimators, you get more \"out of bag folds\" so your OOB performance estimates become more precise.\n",
    "\n",
    "Adding estimators also linearly increases the training time - which makes the added speed of OOB methods that much more crucial when you're tuning a model with a large number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235acc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def oob_vs_n_estimators():\n",
    "    n_experiment_rounds = 10\n",
    "    \n",
    "    oob_mses_16 = [oob_mse(dict(n_estimators=16)) for _ in tqdm(range(n_experiment_rounds))]\n",
    "    oob_mses_64 = [oob_mse(dict(n_estimators=64)) for _ in tqdm(range(n_experiment_rounds))]\n",
    "    oob_mses_256 = [oob_mse(dict(n_estimators=256)) for _ in tqdm(range(n_experiment_rounds))]\n",
    "    \n",
    "    mses = pd.DataFrame(dict(oob_16=oob_mses_16, \n",
    "                             oob_64=oob_mses_64,\n",
    "                             oob_256=oob_mses_256\n",
    "                            ))\n",
    "    \n",
    "    return mses\n",
    "    \n",
    "mses = oob_vs_n_estimators()\n",
    "mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses.std().plot(kind='bar', title='standard deviation of mse estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abc752",
   "metadata": {},
   "source": [
    "# But wait ... there's more!\n",
    "\n",
    "There are even more benefits to using OOB predictions.  We'll potentially discuss those in a future article..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
