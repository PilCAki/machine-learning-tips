{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff34ae21",
   "metadata": {},
   "source": [
    "# Don't Explode - Encode\n",
    "\n",
    "by Phillip Adkins, Principal Machine Learning Science Manager, Microsoft IDEAs \n",
    "\n",
    "\n",
    "\n",
    "#  Read This Before You Start Preprocessing Your Data for Tree Based Models!\n",
    "\n",
    "Preprocessing data is huge part of building a Machine Learning model.  Data in its raw form often can't be fed directly into an algorithm.  Preprocessing enables ML algorithms to ingest and learn from data.\n",
    "\n",
    "There are standard preprocessing techniques which are useful in a variety of contexts.  Techniques such as standardization, one-hot-encoding, missing-value imputation and more are a great way to shape your data up for feeding into and algorith ... unless they're not!  \n",
    "\n",
    "And when you're working with tree-based models - these techniques can vary from useless to harmful.  These techniques can be time wasters, increase the likelihood of errors, and can blow up memory, time and compute requirements while *lowering* model quality! \n",
    "\n",
    "Let's talk about one particular preprocessing technique to think twice about when you're working with tree-based models, and what to do instead.\n",
    "\n",
    "\n",
    "# One-Hot-Encoding - Not So Hot?\n",
    "\n",
    "When working with categorical data, the #1 go-to method for converting categories into something which an ML algorithm can ingest is One-Hot-Encoding.  And it's a great technique!  However there are several reasons why One-Hot-Encoding is a poor choice for tree-based algorithms.\n",
    "\n",
    "1) The number 1 reason is that tree-based algorithms simply don't work very well with one-hot-encodings. Because they work based on partitioning, one-hot encoding forces the decision trees to sequester data points by individual categorical values - there's no way for the model to say \"if country == \"USA\" or \"UK\" then X\".  If it wants to use \"country\", there will have to be a \"USA only\" branch in the tree, a \"UK only\" branch in the tree etc ...\n",
    "\n",
    "2) It can either blow up memory consumption or forces you to work with a sparse matrix\n",
    "\n",
    "3) It alters the shape of your data so the columns no longer 1:1 coincide with your original data frame / table\n",
    "\n",
    "Think about mapping feature importance back to the original features!  It's now very difficult to the point that you might just opt not to do it, or you may be forced to look at your feature importances very differently than you otherwise would have.  \n",
    "\n",
    "For example, instead of a feature importance for \"country\", you'll have a feature importance for \"country=USA\", another one for \"country=UK\" etc - from which it will be hard to determine the relative importance of \"country\" as a whole unless you do some extra math and transformations on top of these fractured feature importances.\n",
    "\n",
    "What if you want to do feature selection using feature importances?  Will you drop individual values of categorical variables from your dataframe?  These will be the first to go when you filter on feature importance if you've done one-hot-encoding.\n",
    "\n",
    "One-hot-encoding makes a mess!\n",
    "\n",
    "4) There's a simpler, more accurate, faster method\n",
    "\n",
    "\n",
    "## Another Class of Encoding\n",
    "\n",
    "There are many methods to encode categoricals.  One-Hot-Encoding, while it has many drawbacks, it ideal in one sense: you can essentially feed it into any algorithm, and the algorithm will have a reasonable chance at extracting signal from the categorical. This is especially true for models which use dot products or matrix multiplications - like linear models, neural networks etc.\n",
    "\n",
    "However, tree models are much more flexible in how they can extract information from a single feature.  Since tree models work by chopping up a given feature into partitions, a tree model is capable of carving the data up into segments which are defined by the categorical without having a one-hot-encoding handed to it.\n",
    "\n",
    "A whole class of encodings is defined by mapping a categorical vector to a single numeric vector.  There are many strategies for doing this, and there's an oppotunity here for a ML Scientist to imbue the categorical representation with some extra usefulness with some clever thinking while engineering features.\n",
    "\n",
    "A good example of this is *target encoding*.\n",
    "\n",
    "With target encoding, we map the categorical to the mean value of the target.  This can be an especially useful encoding method for converting the categorical to something ordinal that is clearly relevant to the predictive task.  \n",
    "\n",
    "A caveat here is that it's easy to overfit if you're not careful.  Some ways around this include: using older historical data to compute the encoding (rather than your current training set), doing stacking etc.\n",
    "\n",
    "However, you might be surprised to find out that a tree-based model can work exceedingly well with encodings that at-a-glance seem like they wouldn't be any good, or that the model would be unlikely to extract signal from.  \n",
    "\n",
    "Here are some examples of these types of encodings:\n",
    "- map categorical to random float\n",
    "- sort categorical alphabetically, map to index in that list (ordinal encoding)\n",
    "- map categorical to its frequency in the data\n",
    "- map categorical to its frequency RANK in the data\n",
    "etc.  \n",
    "\n",
    "Amazingly, tree-based models are totally fine using encodings like this - which has been demonstrated repeatedly in data science competitions and in professional applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7c9ac",
   "metadata": {},
   "source": [
    "# When do these encodings NOT work?\n",
    "\n",
    "Here are some reasons these encodings may not work:\n",
    "\n",
    "1) collisions in the map\n",
    "2) too many categorical values\n",
    "3) model not tuned properly to allow partitioning fine enough to zero in on specific categoricals\n",
    "4) luck - the encoding might just for whatever reason obscure information from the splitting criterion and may prevent the tree from deciding to use the feature even though it's got information content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98613818",
   "metadata": {},
   "source": [
    "# Other Encodings\n",
    "\n",
    "In order to circumvent some of these issues, there are other ways of encoding categoricals to help ensure that information can be extract.\n",
    "\n",
    "There is also \"binary encoding\" - (which is not the same as one-hot-encoding even though one-hot-encoding does produce binary features)\n",
    "\n",
    "Dimensionality reduction applied to the one-hot-encoding - like PCA, or random projection is another alternative that may retain much of the information in the categorical while keeping the representation skinny and dense.\n",
    "\n",
    "Binary encoding and projects-based methods do have the undesirable attribute that they may destroy the alignment between the raw data and the transformed input data, but they do retain density so can still be a good compromise if the other encodings aren't working for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e6b95",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Let's run some quick experiments to demonstrate the effectiveness of some of these methods.\n",
    "\n",
    "This is easy to extend.  For now, we'll test the following 3 methods on a variety of datasets:\n",
    "- one-hot-encoding\n",
    "- ordinal encoding\n",
    "- binary encoding\n",
    "- target encoding\n",
    "- count encoding (frequency encoding)\n",
    "\n",
    "The Category Encoder package's ordinal encoder (which we're using here) maps categoricals to randomly selected integers.  Scikit-learn also has an Ordinal Encoder, but it maps categories to their rank in the alphabetically sorted list of categories.   \n",
    "\n",
    "On each dataset, we do a mini random hyperparam optimization on xgboost to attempt to find good params for each method.\n",
    "\n",
    "We'll then get a look at a table of output scores with which we can compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa6e9e",
   "metadata": {},
   "source": [
    "## Function: Run a Single Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ef9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "class ContinuousToFloatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, continuous_columns):\n",
    "        self.continuous_columns = continuous_columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_continuous = X[self.continuous_columns].astype(np.float64)\n",
    "        return X_continuous\n",
    "\n",
    "def categorical_encoding_experiment(dataset_filename, feature_names, target_name, categoricals,\n",
    "                                    continuous, encoding_method, problem_type):\n",
    "    \n",
    "    def run_experiment(X_train, y_train, X_val, y_val, problem_type, params):\n",
    "        if problem_type == \"classification\":\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                early_stopping_rounds=10,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"logloss\",\n",
    "                verbose=False\n",
    "            )\n",
    "            preds = model.predict(X_val)\n",
    "            return accuracy_score(y_val, preds)\n",
    "\n",
    "        else:\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                early_stopping_rounds=10,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"rmse\",\n",
    "                verbose=False\n",
    "            )\n",
    "            preds = model.predict(X_val)\n",
    "            return mean_squared_error(y_val, preds)\n",
    "\n",
    "    df = pd.read_csv(dataset_filename)\n",
    "\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "\n",
    "    # Define categorical encoding method\n",
    "    if encoding_method == \"one_hot\":\n",
    "        encoder = ce.OneHotEncoder(cols=categoricals)\n",
    "    elif encoding_method == \"ordinal\":\n",
    "        encoder = ce.OrdinalEncoder(cols=categoricals)\n",
    "    elif encoding_method == \"binary\":\n",
    "        encoder = ce.BinaryEncoder(cols=categoricals)\n",
    "    elif encoding_method == \"target\":\n",
    "        encoder = ce.TargetEncoder(cols=categoricals)\n",
    "    elif encoding_method == \"count\":\n",
    "        encoder = ce.CountEncoder(cols=categoricals)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encoding_method\")\n",
    "        \n",
    "    # Define column transformer\n",
    "    preprocessor = ColumnTransformer(transformers=[(\"categorical\", encoder, categoricals)], remainder=\"passthrough\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train, y_train)\n",
    "    X_val = preprocessor.transform(X_val)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    random_search_results = []\n",
    "\n",
    "    for i in range(1):\n",
    "        learning_rate = 0.1#np.random.uniform(0.01, 0.3)\n",
    "        max_depth = 3#np.random.randint(2, 5)\n",
    "        n_estimators = 500# np.random.randint(50, 200)\n",
    "\n",
    "        params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        score = run_experiment(X_train, y_train, X_val, y_val, problem_type, params)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        result = {\n",
    "            \"iteration\": i + 1,\n",
    "            \"params\": params,\n",
    "            \"score\": score,\n",
    "            \"elapsed_time\": elapsed_time\n",
    "        }\n",
    "\n",
    "        random_search_results.append(result)\n",
    "        print(result)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_filename,\n",
    "        \"problem_type\": problem_type,\n",
    "        \"encoding_method\": encoding_method,\n",
    "        \"results\": random_search_results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d910a",
   "metadata": {},
   "source": [
    "## Pull Data, Load It, and Run All Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d243de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing abalone.csv\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 4.889955867904465, 'elapsed_time': 0.19076967239379883}\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 4.882568465268943, 'elapsed_time': 0.22170042991638184}\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 4.958471274702026, 'elapsed_time': 0.2112436294555664}\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 4.882568465268943, 'elapsed_time': 0.24178767204284668}\n",
      "doing titanic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.8028169014084507, 'elapsed_time': 0.09649848937988281}\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.7746478873239436, 'elapsed_time': 0.1018681526184082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.8028169014084507, 'elapsed_time': 0.1271824836730957}\n",
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.8028169014084507, 'elapsed_time': 0.10581469535827637}\n",
      "doing bank.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.9087641692009953, 'elapsed_time': 2.3076682090759277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.9084876969864528, 'elapsed_time': 3.197153091430664}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.9112524191318773, 'elapsed_time': 5.439720630645752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.9095935858446226, 'elapsed_time': 3.8912975788116455}\n",
      "doing mushroom.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 1.0, 'elapsed_time': 4.080474615097046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 1.0, 'elapsed_time': 2.138575792312622}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 1.0, 'elapsed_time': 5.378643989562988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 1.0, 'elapsed_time': 1.2091398239135742}\n",
      "doing adult.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.8664107485604606, 'elapsed_time': 6.174733638763428}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.8714011516314779, 'elapsed_time': 4.378894805908203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.872168905950096, 'elapsed_time': 10.834599256515503}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padkins\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'params': {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}, 'score': 0.872168905950096, 'elapsed_time': 4.939168930053711}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_save_dataset(url, column_names, columns_to_save, filename, sep):\n",
    "    try:\n",
    "        df = pd.read_csv(url, sep=sep)[columns_to_save]\n",
    "    except:\n",
    "        df = pd.read_csv(url, sep=sep, header=None, names=column_names)[columns_to_save]\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "def run_experiment(\n",
    "    dataset_filename,\n",
    "    feature_names,\n",
    "    target_name,\n",
    "    categoricals,\n",
    "    continuous,\n",
    "    problem_type,\n",
    "    encoding_methods\n",
    "):\n",
    "    experiment_results = []\n",
    "    for encoding_method in encoding_methods:\n",
    "        experiment_results.append(\n",
    "            categorical_encoding_experiment(\n",
    "                dataset_filename,\n",
    "                feature_names,\n",
    "                target_name,\n",
    "                categoricals,\n",
    "                continuous,\n",
    "                encoding_method,\n",
    "                problem_type\n",
    "            )\n",
    "        )\n",
    "    return experiment_results\n",
    "\n",
    "def run_all_experiments():\n",
    "    datasets = [\n",
    "        {\n",
    "            \"url\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\",\n",
    "            \"column_names\": [\n",
    "                \"sex\",\n",
    "                \"length\",\n",
    "                \"diameter\",\n",
    "                \"height\",\n",
    "                \"whole_weight\",\n",
    "                \"shucked_weight\",\n",
    "                \"viscera_weight\",\n",
    "                \"shell_weight\",\n",
    "                \"rings\"\n",
    "            ],\n",
    "            \"filename\": \"abalone.csv\",\n",
    "            \"target_name\": \"rings\",\n",
    "            \"categoricals\": [\"sex\"],\n",
    "            \"continuous\": [\n",
    "                \"length\",\n",
    "                \"diameter\",\n",
    "                \"height\",\n",
    "                \"whole_weight\",\n",
    "                \"shucked_weight\",\n",
    "                \"viscera_weight\",\n",
    "                \"shell_weight\"\n",
    "            ],\n",
    "            \"problem_type\": \"regression\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"url\": \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\",\n",
    "            \"column_names\": [\n",
    "                \"Survived\",\n",
    "                \"Pclass\",\n",
    "                \"Name\",\n",
    "                \"Sex\",\n",
    "                \"Age\",\n",
    "                \"Siblings/Spouses Aboard\",\n",
    "                \"Parents/Children Aboard\",\n",
    "                \"Fare\"\n",
    "            ],\n",
    "            \"filename\": \"titanic.csv\",\n",
    "            \"target_name\": \"Survived\",\n",
    "            \"categoricals\": [\"Pclass\", \"Sex\"],\n",
    "            \"continuous\": [\"Age\", \"Siblings/Spouses Aboard\", \"Parents/Children Aboard\", \"Fare\"],\n",
    "            \"problem_type\": \"classification\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://raw.githubusercontent.com/ayan-cs/bank-marketing-uciml/main/bank-full.csv\",\n",
    "            \"column_names\": [\n",
    "                \"age\",\n",
    "                \"job\",\n",
    "                \"marital\",\n",
    "                \"education\",\n",
    "                \"default\",\n",
    "                \"housing\",\n",
    "                \"loan\",\n",
    "                \"contact\",\n",
    "                \"month\",\n",
    "                \"duration\",\n",
    "                \"campaign\",\n",
    "                \"pdays\",\n",
    "                \"previous\",\n",
    "                \"poutcome\",\n",
    "                \"y\"\n",
    "            ],\n",
    "            \"separator\":';',\n",
    "            \"filename\": \"bank.csv\",\n",
    "            \"target_name\": \"y\",\n",
    "            \"categoricals\": [\n",
    "                \"job\",\n",
    "                \"marital\",\n",
    "                \"education\",\n",
    "                \"default\",\n",
    "                \"housing\",\n",
    "                \"loan\",\n",
    "                \"contact\",\n",
    "                \"month\",\n",
    "                \"poutcome\"\n",
    "            ],\n",
    "            \"continuous\": [\n",
    "                \"age\",\n",
    "                \"duration\",\n",
    "                \"campaign\",\n",
    "                \"pdays\",\n",
    "                \"previous\",\n",
    "            ],\n",
    "            \"problem_type\": \"classification\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "            \"column_names\": [\n",
    "                \"class\",\n",
    "                \"cap_shape\",\n",
    "                \"cap_surface\",\n",
    "                \"cap_color\",\n",
    "                \"bruises\",\n",
    "                \"odor\",\n",
    "                \"gill_attachment\",\n",
    "                \"gill_spacing\",\n",
    "                \"gill_size\",\n",
    "                \"gill_color\",\n",
    "                \"stalk_shape\",\n",
    "                \"stalk_root\",\n",
    "                \"stalk_surface_above_ring\",\n",
    "                \"stalk_surface_below_ring\",\n",
    "                \"stalk_color_above_ring\",\n",
    "                \"stalk_color_below_ring\",\n",
    "                \"veil_type\",\n",
    "                \"veil_color\",\n",
    "                \"ring_number\",\n",
    "                \"ring_type\",\n",
    "                \"spore_print_color\",\n",
    "                \"population\",\n",
    "                \"habitat\"\n",
    "            ],\n",
    "            \"filename\": \"mushroom.csv\",\n",
    "            \"target_name\": \"class\",\n",
    "            \"categoricals\": [\n",
    "                \"cap_shape\",\n",
    "                \"cap_surface\",\n",
    "                \"cap_color\",\n",
    "                \"bruises\",\n",
    "                \"odor\",\n",
    "                \"gill_attachment\",\n",
    "                \"gill_spacing\",\n",
    "                \"gill_size\",\n",
    "                \"gill_color\",\n",
    "                \"stalk_shape\",\n",
    "                \"stalk_root\",\n",
    "                \"stalk_surface_above_ring\",\n",
    "                \"stalk_surface_below_ring\",\n",
    "                \"stalk_color_above_ring\",\n",
    "                \"stalk_color_below_ring\",\n",
    "                \"veil_type\",\n",
    "                \"veil_color\",\n",
    "                \"ring_number\",\n",
    "                \"ring_type\",\n",
    "                \"spore_print_color\",\n",
    "                \"population\",\n",
    "                \"habitat\"\n",
    "            ],\n",
    "            \"continuous\": [],\n",
    "            \"problem_type\": \"classification\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            \"column_names\": [\n",
    "                \"age\",\n",
    "                \"workclass\",\n",
    "                \"fnlwgt\",\n",
    "                \"education\",\n",
    "                \"education_num\",\n",
    "                \"marital_status\",\n",
    "                \"occupation\",\n",
    "                \"relationship\",\n",
    "                \"race\",\n",
    "                \"sex\",\n",
    "                \"capital_gain\",\n",
    "                \"capital_loss\",\n",
    "                \"hours_per_week\",\n",
    "                \"native_country\",\n",
    "                \"income\"\n",
    "            ],\n",
    "            \"filename\": \"adult.csv\",\n",
    "            \"target_name\": \"income\",\n",
    "            \"categoricals\": [\n",
    "                \"workclass\",\n",
    "                \"education\",\n",
    "                \"marital_status\",\n",
    "                \"occupation\",\n",
    "                \"relationship\",\n",
    "                \"race\",\n",
    "                \"sex\",\n",
    "                \"native_country\"\n",
    "            ],\n",
    "            \"continuous\": [\n",
    "                \"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"education_num\",\n",
    "                \"capital_gain\",\n",
    "                \"capital_loss\",\n",
    "                \"hours_per_week\"\n",
    "            ],\n",
    "            \"problem_type\": \"classification\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    encoding_methods = [\"binary\", \"ordinal\", \"one_hot\", \"count\"]\n",
    "\n",
    "    all_experiment_results = []\n",
    "    for dataset in datasets:\n",
    "        print('doing', dataset['filename'])\n",
    "        sep = dataset.get('separator', ',')\n",
    "        columns_to_save = dataset['continuous'] + dataset['categoricals'] + [dataset['target_name']]\n",
    "        df = load_and_save_dataset(dataset[\"url\"], dataset['column_names'], columns_to_save, dataset[\"filename\"], sep)\n",
    "        experiment_results = run_experiment(\n",
    "            dataset[\"filename\"],\n",
    "            dataset[\"column_names\"],\n",
    "            dataset[\"target_name\"],\n",
    "            dataset[\"categoricals\"],\n",
    "            dataset[\"continuous\"],\n",
    "            dataset[\"problem_type\"],\n",
    "            encoding_methods\n",
    "        )\n",
    "        all_experiment_results.extend(experiment_results)\n",
    "\n",
    "    return all_experiment_results\n",
    "\n",
    "\n",
    "all_experiment_results = run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bbef7",
   "metadata": {},
   "source": [
    "## Parse and Display Results as Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7641e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding_method</th>\n",
       "      <th>binary</th>\n",
       "      <th>count</th>\n",
       "      <th>one_hot</th>\n",
       "      <th>ordinal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>problem_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abalone.csv</th>\n",
       "      <th>regression</th>\n",
       "      <td>4.889956</td>\n",
       "      <td>4.882568</td>\n",
       "      <td>4.958471</td>\n",
       "      <td>4.882568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult.csv</th>\n",
       "      <th>classification</th>\n",
       "      <td>0.866411</td>\n",
       "      <td>0.872169</td>\n",
       "      <td>0.872169</td>\n",
       "      <td>0.871401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank.csv</th>\n",
       "      <th>classification</th>\n",
       "      <td>0.908764</td>\n",
       "      <td>0.909594</td>\n",
       "      <td>0.911252</td>\n",
       "      <td>0.908488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mushroom.csv</th>\n",
       "      <th>classification</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic.csv</th>\n",
       "      <th>classification</th>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.774648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "encoding_method                binary     count   one_hot   ordinal\n",
       "dataset      problem_type                                          \n",
       "abalone.csv  regression      4.889956  4.882568  4.958471  4.882568\n",
       "adult.csv    classification  0.866411  0.872169  0.872169  0.871401\n",
       "bank.csv     classification  0.908764  0.909594  0.911252  0.908488\n",
       "mushroom.csv classification  1.000000  1.000000  1.000000  1.000000\n",
       "titanic.csv  classification  0.802817  0.802817  0.802817  0.774648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def results_to_dataframe(all_experiment_results):\n",
    "    ''' construct dataframe, \n",
    "    extract best experimental result\n",
    "    for each encoding technique '''\n",
    "    def best_score(experiment):\n",
    "        best = min if experiment['problem_type'] == 'regression' else max if experiment['problem_type'] == 'classification' else None\n",
    "        return best(result['score'] for result in experiment['results' ])\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(all_experiment_results)\n",
    "    df['score'] = [best_score(e) for e in all_experiment_results] \n",
    "\n",
    "    return df\n",
    "\n",
    "df = results_to_dataframe(all_experiment_results)\n",
    "df\n",
    "df.set_index(['dataset', 'problem_type', 'encoding_method'])['score'].unstack('encoding_method')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d8f3a",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "For regression problems, we're computing the mean squared error; lower is better.  \n",
    "\n",
    "For classification: it's \"accuracy\"; higher is better.  \n",
    "\n",
    "** Note: \"accuracy\" is not my favorite metric for understanding how well an ML algo is doing and has plenty of issues, but I think it's fair for comparing multiple algos.  I plan on dressing this up a bit later. \n",
    "\n",
    "According my experience and that of many others as well, \"Count Encoding\" a.k.a. \"Frequency Encoding\" is a very strong and robust technique.  You can see that here - it's consistently the best of the alternate encoding techniques auditioned. \n",
    "\n",
    "Best Results:\n",
    "- abalone: count encoding\n",
    "- adult: count / one-hot tied\n",
    "- bank: one-hot encoding\n",
    "- mushroom: too easy - all 100%\n",
    "- titanic: binary / count / one-hot tied\n",
    "\n",
    "\n",
    "The differences in accuracy here are very small.  On other datasets, it may be larger.\n",
    "\n",
    "The takeaway from this table is that count encoding is typically about as good on average as one-hot, but it has the advantages alluded to above including simplicity of representation.  \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "I highly recommend that you use count/frequency encoding instead of one-hot encoding when working with tree-based models unless you've got a very good reason!\n",
    "\n",
    "# Followup\n",
    "\n",
    "I'd like to repeat these experiments with more datasets and improve the depth of comparisons between methods. \n",
    "\n",
    "There's a lot more to explore here. \n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafada5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
