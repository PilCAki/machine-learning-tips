{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da109e5",
   "metadata": {},
   "source": [
    "# Using Langchain to Query Any Document With an LLM in Less Than 30 Simple Lines Of Code\n",
    "\n",
    "You've learned linear regression as part of the basics for machine learning.  Now, learn question answering with documents as part of the basics of LLM's!\n",
    "\n",
    "With Large Language Models (LLM's) becoming more ubiquitous for ML-driven product design, there are elementary building blocks which any Machine Learning scientist should be familiar with for building out modern, A.I. powered solutions.\n",
    "\n",
    "In particular, interacting with a document is a fundamental component of many types of LLM-driven solution.  Search, CoPilots, Virtual Support Agents etc can be implemented as systems which reference specific documents or information and enable interaction with this information via chat or prompt.\n",
    "\n",
    "Langchain provides a great interface for building on top of Large Langage Models' capabilities.  In particular, it's a piece of cake to use langchain to put together a question answering service for any document, regardless of its length.   \n",
    "\n",
    "The pattern we're discussing today is based on a divide and conquer strategy for documents.  The goal is to be able to ask a question about a document of arbitrary length and to get an answer.  \n",
    "\n",
    "Here's what it looks like:\n",
    "- select a document you'd like to query\n",
    "- break it up into chunks \n",
    "- use a similarity metric to choose relevant chunks which may contain answers to your question\n",
    "- pick the most relevant chunks, and use them to form a prompt which the LLM can directly reference to answer questions\n",
    "- ask the question - receive an answer based on the provided text chunks\n",
    "\n",
    "One might wonder why we're breaking the document into chunks.  If the document was short enough, we could feed it into the prompt and directly ask questions.  Assuming we're working with a longer document, we won't be able to fit it into the prompt.  The chunking strategy works by only filling the prompt with text that is likely to contain the answer to the question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ef30a",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "You'll need to install openai, langchain, tiktoken, faiss and perhaps a few more packages.\n",
    "\n",
    "You'll also need your own OpenAPI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea1ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai_api_key = 'YourAPIKey'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0667c",
   "metadata": {},
   "source": [
    "For this demo, I copy / pasted some documentation from azure automl into a text file. Feel free to use whatever text file you'd like to do this with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519c4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('azure_automl_documentation.txt')\n",
    "document_text = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929edc1e",
   "metadata": {},
   "source": [
    "The strategy we're going to take is to divide the document up into chunks.\n",
    "\n",
    "We'll split the document into overlapping chunks.  I chose a chunk size of 500 and a chunk overlap of 100, but these are params you can play with.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5de5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break document into chunks using text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69985e24",
   "metadata": {},
   "source": [
    "Something to keep in mind when choosing chunk size is: the text that indicates relevance to the question needs to be within the same chunk as the answer to the question.  \n",
    "\n",
    "For example, if you ask \"what is the list of supported ML models\", and the chunk size is 110, you might retrieve a chunk that says \"the list of supported ML models is comprehensive and represents the current state of the art in A.I. included are {text truncated}\" which doesn't actually include the answer!  It'd be hard for the model to answer the question without the full list of ML models being included in the chunk.\n",
    "\n",
    "On the other hand, we don't want chunks so large that that their relevance score gets diluted by irrelevant text.  So - this is a param that needs to be optimized depending on the structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514f117",
   "metadata": {},
   "source": [
    "Now, lets compute embeddings for each of the document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bce826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all document chunks using OpenAI Embeddings API\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "chunk_embeddings = [embeddings.embed_query(chunk.page_content) for chunk in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1a660",
   "metadata": {},
   "source": [
    "Now, convert these into a vector store.  The vector store allows us to retrieve documents similar to a query document we provide.  In this case, we'll be looking for document chunks that are similar to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eeac89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FAISS vector store from the documents and embeddings\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b452b",
   "metadata": {},
   "source": [
    "Here's the main function for querying the document store.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894ed407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top relevant chunks and launch chat session\n",
    "def query_document(question, print_prompt=False):\n",
    "    # Retrieve top relevant chunks using FAISS vector store\n",
    "    retrieved_documents = retriever.get_relevant_documents(question)\n",
    "    relevant_chunks = [document.page_content for document in retrieved_documents]\n",
    "\n",
    "    # Update the prompt with the top relevant chunks\n",
    "    prompt = ('Directly below I will provide documentation for answering questions. ' + \n",
    "              'Then, I will ask a question.  Only use information provided in the documentation.  The question will begin after \"User: \" ' +\n",
    "              'If the answer is not in the documentation provided, respond with \"answer not found\"')\n",
    "    prompt = '\\n\\n'.join(relevant_chunks)\n",
    "    prompt += f'\\n\\nUser: {question}\\n'\n",
    "\n",
    "    # Launch chat session with the updated prompt\n",
    "    '''\n",
    "    Note that we supply a temperature of 0.  \n",
    "    Temperature increases creativity but it also increases randomness.  \n",
    "    We don't want random, creative answers - we just want the cold, hard facts! \n",
    "    '''\n",
    "    chat = ChatOpenAI(temperature=0.0, openai_api_key=openai_api_key, model=\"gpt-4\")\n",
    "    response = chat([HumanMessage(content=prompt)])\n",
    "    \n",
    "    if print_prompt: print('**** PROMPT ****\\n\\n', prompt)\n",
    "    print('**** RESPONSE *****\\n\\n',response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcf7a5",
   "metadata": {},
   "source": [
    "It uses the \"get_relevant_documents\" function to ... get relevant documents!  \n",
    "Then, it concatenates all of those document chunks together and inserts them into a prompt.\n",
    "\n",
    "\n",
    "We could just ask questions on top of the chunks without additional context.  However, in order to avoid having the LLM attempt to invent answers which are not actually in the document, I give it the following setup: \n",
    "\n",
    "\n",
    "> 'Directly below I will provide documentation for answering questions. Then, I will ask a question.  Only use information provided in the documentation. The question will begin after \"User: \" If the answer is not in the documentation provided, respond with \"answer not found\"'\n",
    "\n",
    "\n",
    "Note that I specify gpt-4 here.  The quality of the response varies greatly dependending on which model you choose.  For embeddings / chunk retrieval, it appears we can get away with the default model gpt-3.5-turbo.  \n",
    "\n",
    "More complex gpt models are slower and more expensive, but of course generally yield better results.  \n",
    "\n",
    "In this case, it made sense to use a less expensive model for embedding the numerous chunks, and a more expensive model for the few queries on those chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1db73",
   "metadata": {},
   "source": [
    "# I'll Ask The Questions Here\n",
    "\n",
    "Let's try asking our document a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50865357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** RESPONSE *****\n",
      "\n",
      " Yes, the NLP capability supports end-to-end deep neural network NLP training with the latest pre-trained BERT models. This allows you to leverage the power of BERT for various natural language processing tasks in your automated ML experiments.\n"
     ]
    }
   ],
   "source": [
    "query_document(question=\"Does this support Bert?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbc04b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** RESPONSE *****\n",
      "\n",
      " Yes, AutoML supports ONNX (Open Neural Network Exchange) format. With Azure Machine Learning, you can use automated ML to build a Python model and have it converted to the ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. This allows for better interoperability and performance optimization across different machine learning frameworks.\n"
     ]
    }
   ],
   "source": [
    "query_document(question=\"Does this support ONNX?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89358029",
   "metadata": {},
   "source": [
    "In this case, the answers to both questions appear correct.\n",
    "\n",
    "# Debugging\n",
    "\n",
    "This function also supports printing the prompt for debugging and gaining insight into the LLM's behavior.  \n",
    "\n",
    "For example, you may find that the chunks being returned aren't very relevant.  You may also find that the LLM is hallucinating and isn't basing its responses off the document chunks at all - in which case you'll have to think about doing some prompt engineering and perhaps setting up an evaluation method to quantify the accuracy of the question answering service.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ec4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** PROMPT ****\n",
      "\n",
      " The ONNX runtime also supports C#, so you can use the model built automatically in your C# apps without any need for recoding or any of the network latencies that REST endpoints introduce. Learn more about using an AutoML ONNX model in a .NET application with ML.NET and inferencing ONNX models with the ONNX runtime C# API.\n",
      "\n",
      "Next steps\n",
      "There are multiple resources to get you up and running with AutoML.\n",
      "\n",
      "Tutorials/ how-tos\n",
      "Tutorials are end-to-end introductory examples of AutoML scenarios.\n",
      "\n",
      "See the AutoML package for changing default ensemble settings in automated machine learning.\n",
      "\n",
      "\n",
      "AutoML & ONNX\n",
      "With Azure Machine Learning, you can use automated ML to build a Python model and have it converted to the ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. Learn more about accelerating ML models with ONNX.\n",
      "\n",
      "See how to convert to ONNX format in this Jupyter notebook example. Learn which algorithms are supported in ONNX.\n",
      "\n",
      "The NLP capability supports:\n",
      "\n",
      "End-to-end deep neural network NLP training with the latest pre-trained BERT models\n",
      "Seamless integration with Azure Machine Learning data labeling\n",
      "Use labeled data for generating NLP models\n",
      "Multi-lingual support with 104 languages\n",
      "Distributed training with Horovod\n",
      "Learn how to set up AutoML training for NLP models.\n",
      "\n",
      "Seamlessly integrate with the Azure Machine Learning data labeling capability\n",
      "Use labeled data for generating image models\n",
      "Optimize model performance by specifying the model algorithm and tuning the hyperparameters.\n",
      "Download or deploy the resulting model as a web service in Azure Machine Learning.\n",
      "Operationalize at scale, leveraging Azure Machine Learning MLOps and ML Pipelines capabilities.\n",
      "\n",
      "User: Does this support ONNX?\n",
      "\n",
      "**** RESPONSE *****\n",
      "\n",
      " Yes, AutoML supports ONNX (Open Neural Network Exchange) format. With Azure Machine Learning, you can use automated ML to build a Python model and have it converted to the ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. This allows for better interoperability and performance optimization across different machine learning frameworks.\n"
     ]
    }
   ],
   "source": [
    "query_document(question=\"Does this support ONNX?\", print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045d25a",
   "metadata": {},
   "source": [
    "For this question, it looks like the answer is correct and is mostly supported by the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506f752",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "LLM's are a whole new paradigm for machine learning-based solutions.  There are plenty of amazing tools being developed all the time, and capabilities that weren't accessible with so little effort even a year ago.\n",
    "\n",
    "Make sure to stay up-to-date with patterns and recipes for building amazing apps with A.I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e7c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
